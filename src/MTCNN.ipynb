{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMQGT5E8WXDX"
      },
      "source": [
        "File Structure as follows on Google Collab:\n",
        "\n",
        "*   content/\n",
        "  *   dataset/\n",
        "      *  1/\n",
        "          *  {48 photos from 5025 dataset}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81emeoAz4wLV"
      },
      "source": [
        "Implementation heavily influenced from Tim Esler's pytorch implementation and infer notebook, MTCNN: \n",
        "\n",
        "https://github.com/timesler/facenet-pytorch#performance-comparison-of-face-detection-packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLG64_L9iFhi"
      },
      "source": [
        "pip install facenet-pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzJPVXpcxN2A"
      },
      "source": [
        "#Import MTCNN detector and necessary packages\n",
        "import json\n",
        "from facenet_pytorch import MTCNN\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "workers = 0 if os.name == 'nt' else 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3LfEyHhg-vt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24be6ae7-b028-4491-b375-bb8d6cb42526"
      },
      "source": [
        "#Check for CUDA enables GPU, if not then use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Running on device: {}'.format(device))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX-qLCpo1URM"
      },
      "source": [
        "#Instantiate MTCNN detector\n",
        "mtcnn = MTCNN(\n",
        "    image_size=300, margin=0, min_face_size=20,\n",
        "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6vF7iJF19dt"
      },
      "source": [
        "#Helper function to collate images from dataset into the loader for the MTCNN algorithm\n",
        "def collate_fn(x):\n",
        "  return x[0]\n",
        "\n",
        "dataset = datasets.ImageFolder('/content/dataset/')\n",
        "dataset.idx_to_class = {i:c for c, i in dataset.class_to_idx.items()}\n",
        "loader = DataLoader(dataset, collate_fn=collate_fn, num_workers=workers)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1vN7GZr3dzQ"
      },
      "source": [
        "mtcnn_annotated = {}\n",
        "#Iterate through every image within the loader object\n",
        "for i,x in enumerate(loader):\n",
        "  #Run the mtcnn detector for each image and output bounding boxes, landmarks and confidence\n",
        "  boxes, probs, landmarks = mtcnn.detect(x[0], landmarks=True)\n",
        "  #Display landmarks and bounding boxe on face\n",
        "  fig, ax = plt.subplots(figsize=(16, 12))\n",
        "  ax.imshow(x[0])\n",
        "  ax.axis('off')\n",
        "  for box, landmark in zip(boxes, landmarks):\n",
        "    ax.plot(box[[0, 2]],box[[1, 1]],'r-')\n",
        "    ax.plot(box[[0, 2]],box[[3, 3]],'r-')\n",
        "    ax.plot(box[[2, 2]],box[[1, 3]],'r-')\n",
        "    ax.plot(box[[0, 0]],box[[3, 1]],'r-')\n",
        "    ax.scatter(landmark[:, 0], landmark[:, 1], s=10)\n",
        "\n",
        "  #Take the first bounding box and set of landmarks and convert to float values\n",
        "  boxes=boxes[0]\n",
        "  landmarks=landmarks[0].tolist()\n",
        "  list(np.float_(landmarks))\n",
        "  bb = {}\n",
        "  lm = {}\n",
        "  #Place x,y,w,h values for bounding box in dictionary\n",
        "  bb = {\"x\":float(boxes[0]),\"y\":float(boxes[1]),\"w\":float(boxes[2])-float(boxes[0]),\"h\":float(boxes[3])-(boxes[1])}\n",
        "  #Place landmark coordinates in dictionary\n",
        "  lm = {\"left_eye\": landmarks[0], \"right_eye\": landmarks[1], \"nose\": landmarks[2], \"left_mouth\": landmarks[3], \"right_mouth\": landmarks[4]}\n",
        "  #Add bounding box and landmark coordinates to dictionary for every face id\n",
        "  mtcnn_annotated[i] = {\"bounding_box\":bb,\"landmarks\":lm}\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7AmcZNsZG8B"
      },
      "source": [
        "#Export dictionary as JSON file for further analysis\n",
        "json = json.dumps(mtcnn_annotated)\n",
        "f = open(\"mtcnn_annotated.json\",\"w\")\n",
        "f.write(json)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Gk-Gxzv08fK"
      },
      "source": [
        ""
      ]
    }
  ]
}